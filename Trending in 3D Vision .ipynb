{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending in 3D Vision\n",
    "Deep learning not only provides good optimization techniques on feaure engineering, but also gives us unimaginable possibility to combine our intutions into research that could explore this 3D world better. In the research field of 3D Vision, a lot of excellent research work is happening. It's not enough to simply divide them into stereo vision, multi-view, monucular based 3D, we actually want to sort out research topics that could both connect with and distinguish from 2D vision. People have done paper collections on 3D as [link1](https://github.com/flamato/3D-computer-vision-resources) [link2](https://github.com/imkaywu/awesome-3d-vision-list), but they don't usually give us a good bird view, or most-updated infos. Based on above reasons, we create this special collection by summerizing the general ideas behind some most recent papers in 3D vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single Image 3D  & Unsupervised Monucular Video Depth\n",
    "[[Zhang et al. NIPS 2018](http://genre.csail.mit.edu/papers/genre_nips.pdf)] Learning to Reconstruct Shapes from Unseen Classes [[Project](http://genre.csail.mit.edu/)] [[Code]()]\n",
    "\n",
    "[[Wu et al. ECCV 2018](http://shapehd.csail.mit.edu/papers/shapehd_eccv.pdf)]  Learning Shape Priors for\n",
    "Single-View 3D Completion and Reconstruction [[Project](http://shapehd.csail.mit.edu/)] [[Code]()]\n",
    "\n",
    "[[Niu et al. CVPR 2018](https://kevinkaixu.net/papers/niu_cvpr18_im2struct.pdf)] Im2Struct: Recovering 3D Shape Structure from a Single RGB Image [[Code](https://github.com/chengjieniu/Im2Struct)]\n",
    "\n",
    "[[Zou et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper.pdf)] LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image  [[Code](https://github.com/zouchuhang/LayoutNet)] [[Video](https://www.youtube.com/watch?v=WDzYXRP6XDs&feature=youtu.be)]\n",
    "\n",
    "-------------------------\n",
    "[[Mahj. et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf)] Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints [[Project](https://sites.google.com/view/vid2depth)] [[Code](https://github.com/tensorflow/models/tree/master/research/vid2depth)]\n",
    "\n",
    "[[Yang et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_LEGO_Learning_Edge_CVPR_2018_paper.pdf)] LEGO: Learning Edge with Geometry all at Once by Watching Videos  [[Demo](https://www.youtube.com/watch?v=40-GAgdUwI0)] [[Code](https://github.com/zhenheny/LEGO)]\n",
    "\n",
    "[[Zou et al. ECCV 2018](https://arxiv.org/abs/1809.01649)] DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency  [[Project](http://yuliang.vision/DF-Net/)] [[Code](https://github.com/vt-vl-lab/DF-Net)]\n",
    "\n",
    "(`Note: more papers on 'Monucular Video Depth' in CVPR 2017, 2018, ECCV 2018, along with the Robust Vision Challenge`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perception Beyond Visiable(visible light)\n",
    "[[Tulsiani et al. ECCV 2018](https://arxiv.org/pdf/1807.10264.pdf)] Layer-structured 3D Scene Inference\n",
    "via View Synthesis [[Project](https://shubhtuls.github.io/lsi/)] [[Code](https://github.com/google/layered-scene-inference)]\n",
    "\n",
    "[[Rama. et al. ECCV 2018](https://arxiv.org/abs/1807.11010)] Sidekick Policy Learning for Active Visual Exploration  [[Project](http://vision.cs.utexas.edu/projects/sidekicks/)] [[Code](https://github.com/srama2512/sidekicks)]\n",
    "\n",
    "[[Song et al. CVPR 2018](https://arxiv.org/abs/1712.04569)] Im2Pano3D: Extrapolating 360° Structure and Semantics Beyond the Field of View [[Project](http://im2pano3d.cs.princeton.edu/)] [[Code](https://github.com/shurans/im2pano3d/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pose Estimation \n",
    "\n",
    "#### 3.1 Scene Layout and Object Pose\n",
    "\n",
    "[[Huang et al. NIPS 2018](https://arxiv.org/pdf/1810.13049.pdf)] Cooperative Holistic Scene Understanding: Unifying3D Object, Layout, and Camera Pose Estimation [[Video](https://www.youtube.com/watch?v=kXCugGwnr68)]\n",
    "\n",
    "[[Trem. et al. CoRL 2018](https://arxiv.org/pdf/1809.10790.pdf)] Deep Object Pose Estimation for Semantic Robotic\n",
    "Grasping of Household Objects  [[Project](https://research.nvidia.com/publication/2018-09_Deep-Object-Pose)] [[Code](https://github.com/NVlabs/Deep_Object_Pose)]\n",
    "\n",
    "[[Tulsiani et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Tulsiani_Factoring_Shape_Pose_CVPR_2018_paper.pdf)] Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene [[Project](https://shubhtuls.github.io/factored3d/)] [[Code](https://github.com/shubhtuls/factored3d)]\n",
    "\n",
    "[[Tekin et al. CVPR 2018](https://arxiv.org/pdf/1711.08848.pdf)] Real-Time Seamless Single Shot 6D Object Pose Prediction [[Code](https://github.com/Microsoft/singleshotpose)] [[Supp.](http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3117-supp.pdf)] \n",
    "\n",
    "[[Qi et al. CVPR 2018](https://arxiv.org/pdf/1711.08488.pdf)] Frustum PointNets for 3D Object Detection from RGB-D Data [[Project](http://stanford.edu/~rqi/frustum-pointnets/)] [[Code](https://github.com/charlesq34/frustum-pointnets)]\n",
    "\n",
    "#### 3.2 Body Pose \n",
    "[[Guo et al. ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.pdf)]  Neural Graph Matching Networks for Fewshot 3D Action Recognition (`Pose for action recognition`)\n",
    "\n",
    "[[Groueix et al. ECCV 2018](https://arxiv.org/pdf/1806.05228.pdf)] 3D-CODED : 3D Correspondences by Deep Deformation  [[Project](http://imagine.enpc.fr/~groueixt/3D-CODED/)] [[Code](https://github.com/ThibaultGROUEIX/3D-CODED)]\n",
    "\n",
    "[[Joo et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Total_Capture_A_CVPR_2018_paper.pdf)] Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies [[Project](http://www.cs.cmu.edu/~hanbyulj/totalcapture/)] [[Supp.](http://www.cs.cmu.edu/~hanbyulj/totalcapture/totalBody_camready_supp.pdf)]\n",
    "\n",
    "[[Riza et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf)] \n",
    "Dense Human Pose Estimation In The Wild [[Project](http://densepose.org/)] [[Code](https://github.com/facebookresearch/DensePose)]\n",
    "\n",
    "[[Pavl. et al. CVPR 2018](https://arxiv.org/pdf/1805.04092.pdf)] Learning to Estimate 3D Human Pose and Shape from a Single Color Image [[Project](https://www.seas.upenn.edu/~pavlakos/projects/humanshape/)]\n",
    "\n",
    "#### 3.3 Face Pose\n",
    "[[Moniz et al. NIPS 2018](https://papers.nips.cc/paper/8181-unsupervised-depth-estimation-3d-face-rotation-and-replacement.pdf)] Unsupervised Depth Estimation,\n",
    "3D Face Rotation and Replacement [[Code releasing soon](https://github.com/joelmoniz/DepthNets/)]\n",
    "\n",
    "[[Feng et al. ECCV 2018](https://arxiv.org/pdf/1803.07835v1.pdf)] Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network [[Code](https://github.com/YadiraF/PRNet)] [[Video](https://www.youtube.com/watch?v=tXTgLSyIha8&feature=youtu.be)]\n",
    "\n",
    "[[Genova et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Genova_Unsupervised_Training_for_CVPR_2018_paper.pdf)] Unsupervised Training for 3D Morphable Model Regression  [[Project]()] [[Code](https://github.com/google/tf_mesh_renderer)]\n",
    "\n",
    "[[Deng et al. CVPR 2018](https://arxiv.org/pdf/1712.04695.pdf)] UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition \n",
    "\n",
    "#### 3.4 Hand Pose\n",
    "[[Tsoli et al. ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/papers/Aggeliki_Tsoli_Joint_3D_tracking_ECCV_2018_paper.pdf)] Joint 3D Tracking of a Deformable Object in Interaction with a Hand\n",
    "\n",
    "[[Ge et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf)] Hand PointNet: 3D Hand Pose Estimation using Point Sets  [[Project](https://sites.google.com/site/geliuhaontu/home/cvpr2018)] [[Code](https://sites.google.com/site/geliuhaontu/HandPointNet.zip?attredirects=0&d=1)] [[VIdeo](https://youtu.be/-eiZYOo8cWc)]\n",
    "\n",
    "[[Baek et al. CVPR 2018](https://arxiv.org/abs/1805.04497)] Augmented skeleton space transfer for depth-based hand pose estimation\n",
    "\n",
    "(`Note: more papers on 'Hand Pose Estimation' in CVPR 2018, ECCV 2018`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Disentangle Representations in 3D\n",
    "[[Yao et al. NIPS 2018](https://arxiv.org/pdf/1808.09351.pdf)] 3D-Aware Scene Manipulation via Inverse Graphics  [[Project](http://3dsdn.csail.mit.edu/)] \n",
    "\n",
    "[[Smith et al. NIPS 2018](https://papers.nips.cc/paper/7883-multi-view-silhouette-and-depth-decomposition-for-high-resolution-3d-object-representation.pdf)] Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation  [[Project](https://sites.google.com/site/mvdnips2018)] \n",
    "\n",
    "[[Zhu et al. NIPS 2018](https://papers.nips.cc/paper/7297-visual-object-networks-image-generation-with-disentangled-3d-representations.pdf)] Visual Object Networks: Image Generation with\n",
    "Disentangled 3D Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Unsupervised Key Points Detection\n",
    "[[Suwa. et al. NIPS 2018](https://arxiv.org/pdf/1807.03146.pdf)] Discovery of Latent 3D Keypoints via\n",
    "End-to-end Geometric Reasoning  [[Project](https://keypointnet.github.io/)] [[Code](https://github.com/tensorflow/models/tree/master/research/keypointnet)]\n",
    "\n",
    "[[Zhou et al. ECCV 2018](https://arxiv.org/pdf/1712.05765.pdf)] Unsupervised Domain Adaptation for 3D\n",
    "Keypoint Estimation via View Consistency [[Code](https://github.com/xingyizhou/3DKeypoints-DA)]  [[Results](https://drive.google.com/file/d/1UtlL7moKtNoVGyqWGRn8_c_57dwiqlVm/view)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Point Cloud(PCL) Processing \n",
    "#### 6.1 Neural Network for 3D data\n",
    "[[Tata. et al. CVPR 2018](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0144.pdf)] Tangent Convolutions for Dense Prediction in 3D [[Code](https://github.com/tatarchm/tangent_conv)]\n",
    "\n",
    "[[Su et al. CVPR 2018](https://arxiv.org/abs/1802.08275)] SPLATNet: Sparse Lattice Networks for Point Cloud Processing \n",
    "[[Project](http://siyuanhuang.com/cooperative_parsing/main.html)] [[Code](https://github.com/NVlabs/splatnet)]\n",
    "\n",
    "[[Qi et al. NIPS 2017](https://arxiv.org/abs/1706.02413)] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space  [[Project](http://stanford.edu/~rqi/pointnet2/)] [[Code](https://github.com/charlesq34/pointnet2)]\n",
    "\n",
    "[[Weiler et al. NIPS 2018](https://papers.nips.cc/paper/8239-3d-steerable-cnns-learning-rotationally-equivariant-features-in-volumetric-data.pdf)] 3D Steerable CNNs: Learning Rotationally\n",
    "Equivariant Features in Volumetric Data  [[Project]()] [[Code](https://github.com/mariogeiger/se3cnn)]\n",
    "\n",
    "[[Sung et al. NIPS 2018](https://papers.nips.cc/paper/7330-deep-functional-dictionaries-learning-consistent-semantic-structures-on-3d-models-from-functions.pdf)] Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions  [[Project]()] [[Code](https://github.com/mhsung/deep-functional-dictionaries)]\n",
    "\n",
    "\n",
    "#### 6.2 3D Registration & Rendering\n",
    "[[Nguy. et al. NIPS 2018](https://papers.nips.cc/paper/8014-rendernet-a-deep-convolutional-network-for-differentiable-rendering-from-3d-shapes.pdf)] RenderNet: A deep convolutional network for\n",
    "differentiable rendering from 3D shapes\n",
    "\n",
    "[[Kim et al. ECCV 2018](https://arxiv.org/pdf/1807.02587.pdf)] Fast and Accurate Point Cloud Registration using Trees of Gaussian Mixtures  [[Project](https://research.nvidia.com/publication/2018-09_HGMM-Registration)] [[Video](https://www.youtube.com/watch?v=Bczht9CspiY)]\n",
    "\n",
    "[[Sung et al. Siggraph Asia 2017](https://arxiv.org/abs/1708.01841)] ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling  [[Project](https://mhsung.github.io/complement-me.html)] [[Code](https://github.com/mhsung/complement-me)]\n",
    "\n",
    "[[Zhu et al. SIGGRAPH Asia 2018]()] SCORES: Shape Composition with Recursive Substructure Priors  [[Project](https://kevinkaixu.net/projects/scores.html)] [[Code](https://kevinkaixu.net/projects/scores.html#code)]\n",
    "\n",
    "[[Este. et al. 3DV 2018](https://vision.in.tum.de/_media/spezial/bib/estellers2018.pdf)] Robust Fitting of Subdivision Surfaces for Smooth Shape Analysis  [[Project]()] [[Code](https://bitbucket.org/ginie/subdivision_surfaces_3dv2018/src/master/)]\n",
    "\n",
    "[[Kato et al. CVPR 2018](https://arxiv.org/abs/1711.07566)] Neural 3D Mesh Renderer  [[Project](http://hiroharu-kato.com/projects_en/neural_renderer.html)] [[Code](https://github.com/hiroharu-kato/neural_renderer)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SLAM today\n",
    "#### 7.1 3D Reconstruction & SLAM\n",
    "[[Shi et al. ECCV 2018](https://arxiv.org/pdf/1803.08407)] PlaneMatch: Patch Coplanarity Prediction for\n",
    "Robust RGB-D Reconstruction  [[Project](http://www.yifeishi.net/planematch.html)] [[Code](https://github.com/yifeishi/PlaneMatch)]\n",
    "\n",
    "[[Bloesch et al. CVPR 2018](https://arxiv.org/abs/1804.00874)] CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM [[Project](http://www.imperial.ac.uk/dyson-robotics-lab/projects/codeslam/)] [[Video](https://www.youtube.com/watch?v=PbSggzaZWAQ&t=1s)]\n",
    "\n",
    "[[Berg. et al. ICRA 2018](https://arxiv.org/abs/1710.02081)] Online Photometric Calibration of Auto Exposure Video for Realtime Visual Odometry and SLAM [[Project](https://vision.in.tum.de/research/vslam/photometric-calibration)] [[Code](https://vision.in.tum.de/research/vslam/photometric-calibration)]\n",
    "\n",
    "#### 7.2 Object-aware SLAM\n",
    "[[Rünz et al. ISMAR 2018](https://arxiv.org/pdf/1804.09194.pdf)] MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects [[Project](http://visual.cs.ucl.ac.uk/pubs/maskfusion/index.html)] [[Code](https://github.com/martinruenz/maskfusion)] [[Demo](http://visual.cs.ucl.ac.uk/pubs/maskfusion/MaskFusion.webm)]\n",
    "\n",
    "[[McCo. et al. 3DV 2018](https://www.doc.ic.ac.uk/~sleutene/publications/fusion_plusplus_3dv_camera_ready.pdf)] Fusion++: Volumetric Object-Level SLAM [[Video](https://www.youtube.com/watch?v=2luKNC03x4k&feature=youtu.be)]\n",
    "\n",
    "[[Zhou et al. ECCV 2018](https://arxiv.org/pdf/1808.01900.pdf)] DeepTAM: Deep Tracking and Mapping  [[Project](https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/)] [[Code](https://github.com/lmb-freiburg/deeptam)]\n",
    "\n",
    "#### 7.3 3D Photography\n",
    "[[Hedman et al. SIGGRAPH](http://visual.cs.ucl.ac.uk/pubs/instant3d/instant3d_siggraph_2018.pdf)] Instant 3D Photography  [[Project](http://visual.cs.ucl.ac.uk/pubs/instant3d/)] [[Code](http://visual.cs.ucl.ac.uk/pubs/instant3d/implementation_details.pdf)]\n",
    "\n",
    "[[Chen et al. ECCV 2018](http://gychen.org/PS-FCN/)] PS-FCN: A Flexible Learning Framework for Photometric Stereo [[Project](http://gychen.org/PS-FCN/)] [[Code](https://github.com/guanyingc/PS-FCN)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Misc:\n",
    "### Workshops\n",
    "[Bridge to CVPR 3D workshop](https://bridgesto3d.github.io/#schedule) \n",
    "\n",
    "### Dataset 2018\n",
    "[[Trem. et al. CVPR 2018 Workshop](https://research.nvidia.com/publication/2018-06_Falling-Things)] Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation [[Dataset](https://drive.google.com/open?id=1y4h9T6D9rf6dAmsRwEtfzJdcghCnI_01)]\n",
    "\n",
    "[[Sun et al. CVPR 2018](https://arxiv.org/pdf/1804.04610.pdf)] Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling [[Dataset](https://github.com/xingyuansun/pix3d)]\n",
    "\n",
    "[[Muel. et al. CVPR 2018](https://arxiv.org/abs/1712.01057)] GANerated Hands Dataset  [[Dataset](http://handtracker.mpi-inf.mpg.de/projects/GANeratedHands/GANeratedDataset.htm)]\n",
    "\n",
    "SUMO challenge dataset [[Dataset](https://sumochallenge.org/)] [[Code](https://github.com/facebookresearch/sumo-challenge)]\n",
    "### More resources\n",
    "Amesome 3D machine learning collection [here](https://github.com/timzhang642/3D-Machine-Learning)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
